{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a540231",
   "metadata": {},
   "source": [
    "# Sychronisation de la copie de HAL\n",
    "1. Identification des notices supprimées depuis la dernière synchro\n",
    "    * interrogation en json pour récupérer tous les docid et HalID du jour\n",
    "        * format du nom du fichier : Tous_Les_docids_du_jour_AAAA-MM-JJ.csv (exemple : Tous_Les_docids_du_jour_2024-08-02.csv)\n",
    "    * comparaison avec la liste récupérée la veille\n",
    "        * on ne compare pas les docid, mais les halID : lorsqu'une nouvelle version est créée, le docid de la version précédente disparaît de l'API, on peut donc croire que c'est une suppression. Si une notice est supprimée, son halID disparaît, toutes versions confondues. Donc on compare les halID. Si un halID n'est plus présent dans la liste du jour par rapport à la liste de la veille, c'est que la notice est supprimée.\n",
    "        * le fichier de la veille aura le même format de nom. par exemple : Tous_Les_docids_du_jour_2024-08-01.csv (note : la première liste a été élaborée avec le script Premiere_copie_Hal_DATADCIS.ypnb)\n",
    "        * Création d'un fichier des notices supprimées\n",
    "        * nom du fichier : fichiers_a_supprimer.csv\n",
    "    * Suppression de tout fichier dont le nom contient le halID, dans les dossiers contenant les notices xml et les fichiers associes\n",
    "        * nom des dossiers : /data/Notices_Xml-tei_de_Hal et ../pdf_de_hal\n",
    "\n",
    "\n",
    "\n",
    "2. Synchronisation de la copie de HAL\n",
    "    (voir script Synchro_Deuxieme_Partie_HAL_DATADCIS)\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf579e9",
   "metadata": {},
   "source": [
    "## Synchronisation quotidienne : identification des notices à supprimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99efb10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "## Récupération des docid pour comparaison avec la veille\n",
    "## objectif : identifier les notices supprimées\n",
    "################################################################\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "# Obtenir la date actuelle\n",
    "date_extraction_current = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Définition des variables\n",
    "compteur = 0\n",
    "Les_docids = []\n",
    "\n",
    "## Spécifier le répertoire de log\n",
    "log_directory = '/data/log/'\n",
    "## Créer le répertoire s'il n'existe pas\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "## Déterminer le chemin du dossier DocidDeHAL (au même niveau que le dossier courant)\n",
    "docid_folder ='/data/DocidDeHAL'\n",
    "# Créer le répertoire si nécessaire\n",
    "os.makedirs(docid_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Construire le nom du fichier de log avec le chemin complet\n",
    "log_file = os.path.join(log_directory, date_extraction_current + '_log_synchro_part1_hal.txt')\n",
    "\n",
    "# Configuration du logger\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "#Définition du nom du fichier\n",
    "fichier_des_docids_current = os.path.join(docid_folder,f'Tous_Les_docids_du_jour_{date_extraction_current}.csv')\n",
    "\n",
    "\n",
    "\n",
    "# # Si le fichier existe déjà, sortir du script.\n",
    "# if os.path.exists(fichier_des_docids_current):\n",
    "#     logging.info(f\"Le fichier {fichier_des_docids_current} existe déjà. Arrêt du script.\")\n",
    "#     print(f\"Le fichier {fichier_des_docids_current} existe déjà. Arrêt du script.\")\n",
    "#     raise StopExecution\n",
    "\n",
    "#### Processus de récupération de la liste des notices présentes dans HAL à la date du jour\n",
    "\n",
    "# URL de base de l'API\n",
    "base_url = \"https://api.archives-ouvertes.fr/search\"\n",
    "\n",
    "query = \"*\"\n",
    "\n",
    "# Paramètres de la requête\n",
    "params = {\n",
    "    \"q\": query,\n",
    "    \"wt\": \"json\",\n",
    "    \"fl\": \"docid, halId_s,uri_s,submitType_s\",\n",
    "    \"rows\": 10000,\n",
    "    \"sort\": \"docid asc\"\n",
    "}\n",
    "\n",
    "#url entière : https://api.archives-ouvertes.fr/search/INRIA2?q=*&rows=10000&wt=json&fl=docid,halId_s,uri_s,submitType_s&sort_dodicd asc\n",
    "\n",
    "\n",
    "# Initialisation du cursorMark (qui permet de réitérer la requête jusqu'à la fin des réponses de l'API)\n",
    "cursor_mark = \"*\"\n",
    "previous_cursor_mark = None\n",
    "\n",
    "\n",
    "while cursor_mark != previous_cursor_mark:\n",
    "    # Mise à jour du cursorMark\n",
    "    params[\"cursorMark\"] = cursor_mark\n",
    "    #print(f\"CursorMark: {cursor_mark}\")\n",
    "    compteur += 1\n",
    "\n",
    "    # Envoi de la requête GET\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        results_json = response.json()\n",
    "        docs = results_json['response']['docs']\n",
    "        Nbe_depots = results_json['response']['numFound']\n",
    "        #print({Nbe_depots})\n",
    "        for doc in docs:\n",
    "            docid = doc['docid']\n",
    "            halId = doc['halId_s']\n",
    "            uri = doc['uri_s']\n",
    "            submitType = doc['submitType_s']\n",
    "            Les_docids.append([docid, halId, uri,submitType])\n",
    "            #print(f\"DocID: {docid}, URI: {uri}\")\n",
    "\n",
    "        next_cursor_mark = results_json.get('nextCursorMark')\n",
    "        if not next_cursor_mark:\n",
    "            break\n",
    "\n",
    "    # Mise à jour du cursorMark pour la prochaine itération\n",
    "    previous_cursor_mark = cursor_mark\n",
    "    cursor_mark = next_cursor_mark\n",
    "\n",
    "    #if compteur > 500:  # Limite arbitraire pour les tests\n",
    "        #break\n",
    "\n",
    "    # Pause pour éviter de surcharger l'API\n",
    "    time.sleep(0.1)\n",
    "\n",
    "\n",
    "# Création du DataFrame et exportation en CSV\n",
    "df_current = pd.DataFrame(Les_docids, columns=['docID', 'halId', 'uri','submitType_s'])\n",
    "\n",
    "df_current.to_csv(fichier_des_docids_current, index=False)\n",
    "\n",
    "logging.info(f\"Terminé.  {len(df_current)} notices trouvées.\")\n",
    "print(f\"Terminé.  {len(df_current)} notices trouvées.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Comparaison du fichier du jour et de celui de la veille\n",
    "# Enregistrement du fichier des notices à supprimer\n",
    "# On a aussi une liste de notices ajoutées, mais on n'utilise pas cette information pour l'instant.\n",
    "##################################################################\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "print(f\"étape de comparaison des listes quotidiennes\")\n",
    "\n",
    "# Obtenir la date actuelle\n",
    "date_extraction_current = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Obtenir la date de la veille\n",
    "date_extraction_previous = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "## Déterminer le chemin du dossier DocidDeHAL (au même niveau que le dossier courant)\n",
    "docid_folder ='/data/DocidDeHAL'\n",
    "suppressed_folder ='/data/Suppressions'\n",
    "\n",
    "\n",
    "\n",
    "# Créer le répertoire si nécessaire\n",
    "os.makedirs(docid_folder, exist_ok=True)\n",
    "os.makedirs(suppressed_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "#Création de variables contenant les fichiers à comparer\n",
    "fichier_des_docids_previous = os.path.join(docid_folder,f'Tous_Les_docids_du_jour_{date_extraction_previous}.csv')\n",
    "fichier_des_docids_current = os.path.join(docid_folder,f'Tous_Les_docids_du_jour_{date_extraction_current}.csv')\n",
    "\n",
    "# Infos pour le fichier de log\n",
    "logging.info(f\"date du jour : {date_extraction_current}\")\n",
    "logging.info(f\"fichier du jour: {fichier_des_docids_current}\")\n",
    "logging.info(f\"fichier de la veille: {fichier_des_docids_previous}\")\n",
    "\n",
    "# Lire les fichiers CSV dans des DataFrames\n",
    "df_previous = pd.read_csv(fichier_des_docids_previous, dtype=str)\n",
    "#df2 = df_current\n",
    "df_current = pd.read_csv(fichier_des_docids_current, dtype=str)\n",
    "\n",
    "\n",
    "# Créer une copie (exigence Notebook)\n",
    "df_previous = df_previous.copy()  \n",
    "df_current = df_current.copy()  \n",
    "\n",
    "# Convertir les colonnes 'halID' en chaînes de caractères pour assurer la compatibilité\n",
    "df_previous['halId'] = df_previous['halId'].astype(str)\n",
    "df_current['halId'] = df_current['halId'].astype(str)\n",
    "\n",
    "\n",
    "# Fusionner les deux DataFrames sur la colonne 'halId'\n",
    "\n",
    "merged_df = pd.merge(df_previous, df_current, on='halId', how='outer', indicator=True)\n",
    "\n",
    "\n",
    "# Identifier les enregistrements qui sont uniquement dans df_previous et donc qui ont disparu le jour suivant\n",
    "df_previous_only = merged_df[merged_df['_merge'] == 'left_only'].copy()\n",
    "\n",
    "\n",
    "# Identifier les enregistrements qui sont uniquement dans df_current et donc qui sont apparues depuis la veille\n",
    "df_current_only = merged_df[merged_df['_merge'] == 'right_only'].copy()\n",
    "\n",
    "# Pour fichier de log, indication des résultats de la comparaison\n",
    "logging.info(f\"Nombre d'enregistrements supprimés : {len(df_previous_only)}\")\n",
    "logging.info(f\"Nombre d'enregistrements nouveaux : {len(df_current_only)}\")\n",
    "\n",
    "df_previous_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d40751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docID_x</th>\n",
       "      <th>halId</th>\n",
       "      <th>uri_x</th>\n",
       "      <th>submitType_s_x</th>\n",
       "      <th>docID_y</th>\n",
       "      <th>uri_y</th>\n",
       "      <th>submitType_s_y</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [docID_x, halId, uri_x, submitType_s_x, docID_y, uri_y, submitType_s_y, _merge]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_current_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#suppression de la colonne de résultat de comparaison (_merge : both, left-only, right-only)\n",
    "df_previous_only = df_previous_only.drop(columns=['_merge'])\n",
    "df_current_only = df_current_only.drop(columns=['_merge'])\n",
    "\n",
    "# Renommage des colonnes\n",
    "df_previous_only = df_previous_only.rename(columns={'docID_x': 'docID', 'halId_x': 'halId', 'uri_x': 'uri', 'submitType_s_x': 'submitType_s'})\n",
    "df_current_only = df_current_only.rename(columns={'docID_x': 'docID', 'uri_x': 'uri', 'submitType_s_x': 'submitType_s'})\n",
    "\n",
    "# On ne garde que les colonnes qui nous intéressent\n",
    "df_previous_only = df_previous_only[['docID', 'halId', 'uri', 'submitType_s']]\n",
    "df_current_only = df_current_only[['docID', 'halId', 'uri', 'submitType_s']]\n",
    "\n",
    "\n",
    "# Enregistrer les références des notices supprimées dans HAL dans un fichier CSV\n",
    "fichier_suppr = os.path.join(suppressed_folder,f'{date_extraction_current}_fichiers_a_supprimer.csv')\n",
    "\n",
    "df_previous_only.to_csv(fichier_suppr, index=False)\n",
    "\n",
    "# Si nécessaire, on peut enregistrer la liste des nouvelles notices en décommentant la ligne suivante:\n",
    "#df_current_only.to_csv('notices_nouvelles.csv', index=False)\n",
    "\n",
    "# Message indiquant la fin du traitement\n",
    "print(f\"{fichier_suppr} enregistré.\")\n",
    "logging.info(f\"{fichier_suppr} enregistré.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4f002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***étape de suppressions***\n",
      "Terminé\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "# Suppression des notices identifées comme supprimées dans HAL\n",
    "# Suppression des fichiers correspondants\n",
    "# méthode : on supprime tout fichier dont le nom contient le halID concerné\n",
    "##########################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(f\"***étape de suppressions***\")\n",
    "\n",
    "# Lire le fichier des notices à supprimer dans un dataframe\n",
    "df_suppr = pd.read_csv(fichier_suppr)\n",
    "\n",
    "# Vérifier si le DataFrame est vide, si oui, on s'arrête.\n",
    "if df_suppr.empty:\n",
    "    logging.info(f\"Fichier de notices à supprimer vide\")\n",
    "    exit\n",
    "\n",
    "# Répertoire contenant les fichiers PDF\n",
    "pdf_directory = '/data/pdf_de_hal/'\n",
    "# Répertoire contenant les fichiers xml\n",
    "xml_directory = '/data/Notices_Xml-tei_de_Hal/'\n",
    "\n",
    "\n",
    "################\n",
    "## Suppression des fichiers attachés\n",
    "################\n",
    "# Lire les valeurs de halId des notices et fichiers à supprimer\n",
    "hal_ids_to_delete = df_suppr['halId'].tolist()\n",
    "\n",
    "# Lister tous les fichiers dans les sous-répertoires du répertoire des PDF\n",
    "for root, dirs, files in os.walk(pdf_directory):\n",
    "    for filename in files:\n",
    "        if any(hal_id in filename for hal_id in hal_ids_to_delete):\n",
    "            file_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                logging.info(f\"Supprimé : {file_path}\")\n",
    "            except Exception as e:\n",
    "                logging.info(f\"Erreur lors de la suppression de {file_path}: {e}\")\n",
    "\n",
    "\n",
    "################\n",
    "## Suppression des notices XML\n",
    "################\n",
    "# Lister tous les fichiers dans le répertoire des notices xml\n",
    "for filename in os.listdir(xml_directory):\n",
    "    # Vérifier si le nom de fichier contient l'une des valeurs de halId\n",
    "    if any(hal_id in filename for hal_id in hal_ids_to_delete):\n",
    "        file_path = os.path.join(xml_directory, filename)\n",
    "        try:\n",
    "            # Supprimer le fichier\n",
    "            os.remove(file_path)\n",
    "            logging.info(f\"Supprimé : {file_path}\")\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Erreur lors de la suppression de {file_path}: {e}\")\n",
    "\n",
    "print(\"Terminé\")\n",
    "\n",
    "# Appeler shutdown pour fermer les handlers du fichier de log\n",
    "logging.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
