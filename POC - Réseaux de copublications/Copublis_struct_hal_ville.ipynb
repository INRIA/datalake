{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a540231",
   "metadata": {},
   "source": [
    "# Collection: copublications internationales (UE et hors UE)\n",
    "* demande interne INRIA (27/07/2025)\n",
    "* Réalisation du script (adaptation d'un ancien script) : Kumar Guha (Data DCIS/Inria)\n",
    "* Date 27/02/2025, modifié le 29/08/2025. Denière version : 15/09/2025.\n",
    "\n",
    "## Choix\n",
    "* On ne retient que la première affiliation de chaque auteur (pas les niveaux supérieurs : exemple : Boston University School of Medicine et pas Boston University).\n",
    "* Si un auteur rattaché à une structure française est aussi rattaché à une structure étrangère, on ne retient pas cet auteur pour compter une copublication internationale.\n",
    "* Si un auteur de la structure Inria recherchée est aussi affilié à une autre strucutre étrangère, celle-ci n'est pas mentionnée.\n",
    "\n",
    "\n",
    "## Étapes\n",
    "* Extraire les publications des équipes concernées.\n",
    "* identifier les publications dont les auteurs sont affiliés à un organisme étranger (hors France et DOM TOM)\n",
    "* On crée des listes d'identifiants uniques pour les affiliations FR, Union Européenne et hors UE.\n",
    "    *  on exclut les organismes étrangers dont les auteurs sont aussi affiliés à une structure FR\n",
    "    *  on exclut les affiliations en double pour une même publication\n",
    "    *  nettoyage des données.\n",
    "* Génération d'un fichier Excel avec : chiffres, liste des publications, liste des organismes étrangers copubliants\n",
    "* Première identification de la ville d'après les données de nom de structure et d'adresse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c08c50",
   "metadata": {},
   "source": [
    "## Extraction des publications de HAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd12968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "###############################################################\n",
    "## Extraction des publications de HAL\n",
    "##############################################\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from lxml import etree\n",
    "import gc \n",
    "import locale\n",
    "import re\n",
    "\n",
    "###############################################################################################\n",
    "##################### variables à modifier avant lancement du script ##########################\n",
    "###############################################################################################\n",
    "\n",
    "###########################################################\n",
    "# Définir la structure recherchée:\n",
    "###########################################################\n",
    "nom_collection = \"INRIA\" # il s'agit des publications des équipes Inria de tous les centres.\n",
    "\n",
    "# Identifiant de la structure \"de réference\" dont on analyse les publications (exemple Centre Inria de Rennes : 419153)\n",
    "#(à chercher dans Aurehal : https://aurehal.archives-ouvertes.fr/structure/index)\n",
    "id_aurehal_de_la_structure = \"34586\" #34586 = Centre Inria Université Côte d'Azur\n",
    "nom_de_la_structure = \"Sophia\"\n",
    "\n",
    "# Structures à exclure\n",
    "# Galinette, Stack (id Aurehal :1088569,495900, 1088566, 525233 )\n",
    "equipes_a_exclure = []\n",
    "\n",
    "##########################################\n",
    "# Définir la période recherchée\n",
    "###########################################\n",
    "annee_debut = 2018\n",
    "annee_fin = 2024 # indiquer la même année si la recherche porte sur une seule année\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "##################### script ########################################\n",
    "#####################################################################\n",
    "# Obtenir la date actuelle\n",
    "date_extraction_current = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "## Spécifier le répertoire de log\n",
    "log_directory = '../log/'\n",
    "## Créer le répertoire s'il n'existe pas\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "# Configuration du logger\n",
    "log_file = date_extraction_current + '__international_publications_log.txt'\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Configurer la localisation en français\n",
    "locale.setlocale(locale.LC_TIME, \"French_France.1252\")\n",
    "\n",
    "# La période est définie par les années saisies dans les variables au-dessus du script\n",
    "\n",
    "# periode = \"[\" + annee_debut + \" TO \" + annee_fin + \"]\"\n",
    "\n",
    "# variables cumulatives\n",
    "\n",
    "all_dataex = {}\n",
    "all_datafr = {}\n",
    "all_datapubli = []\n",
    "params = {}\n",
    "pas = 3\n",
    "\n",
    "for start_year in range(annee_debut, annee_fin + 1, pas):\n",
    "    end_year = min(start_year + pas - 1, annee_fin)\n",
    "    periode = f\"[{start_year} TO {end_year}]\"\n",
    "    \n",
    "    print(f\"▶️ Traitement de la période : {periode}\")\n",
    "    \n",
    "    params[\"q\"] = f\"publicationDateY_i:{periode}\"\n",
    "    \n",
    "    # Réinitialise tes variables internes ici si besoin :\n",
    "    cursor_mark = \"*\"\n",
    "    previous_cursor_mark = None\n",
    "    compteur = 0\n",
    "    partenaire = 0\n",
    "    dataex = []\n",
    "    datafr = []\n",
    "    datapubli = []\n",
    "    unique_org_ex = {}\n",
    "    unique_org_fr = {}\n",
    "\n",
    "\n",
    "    # Fonction permettant de réessayer s'il n'y a pas de réponse\n",
    "    def fetch_with_retry(url, params=None, max_retries=3, delay=2):\n",
    "        \"\"\"\n",
    "        Effectue une requête GET avec plusieurs tentatives en cas d'échec.\n",
    "        \n",
    "        Args:\n",
    "            url (str): L'URL de la requête.\n",
    "            params (dict, optional): Paramètres de requête.\n",
    "            max_retries (int): Nombre maximal de tentatives.\n",
    "            delay (int): Temps d'attente entre chaque tentative (en secondes).\n",
    "\n",
    "        Returns:\n",
    "            Response object si la requête réussit, sinon None.\n",
    "        \"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, params=params, timeout=10)  # Timeout pour éviter les blocages\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    return response  # Succès\n",
    "                \n",
    "                print(f\"⚠️ Tentative {attempt + 1} échouée ({response.status_code}). Nouvelle tentative...\")\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"⏳ Erreur réseau ({e}), tentative {attempt + 1}...\")\n",
    "\n",
    "\n",
    "    #### Processus de récupération de la liste des notices présentes dans HAL pour la période spécifiée\n",
    "    # URL de base de l'API\n",
    "    base_url = f\"https://api.archives-ouvertes.fr/search/{nom_collection}?\"\n",
    "\n",
    "    # Paramètres de la requête : les résultats sont traités un par un en xml-tei\n",
    "    params = {\n",
    "        \"q\": f\"publicationDateY_i:{periode}\",\n",
    "        \"fq\": f\"structId_i:{id_aurehal_de_la_structure}\",\n",
    "        \"wt\": \"xml-tei\",\n",
    "        \"rows\": 1,\n",
    "        \"sort\": \"docid asc\"\n",
    "    }\n",
    "\n",
    "    # https://api.archives-ouvertes.fr/search/INRIA2?q=publicationDateY_i:[2019%20TO%202024]&fq=structId_i:(413916%20OR%20526070%20OR%20526181%20OR%20521735%20OR%20521714)&wt=xml-tei&rows=100&sort=docid%20asc\n",
    "\n",
    "    # Initialisation du cursorMark (qui permet de réitérer la requête jusqu'à la fin des réponses de l'API)\n",
    "    cursor_mark = \"*\"\n",
    "    previous_cursor_mark = None\n",
    "\n",
    "    # Définition des variables\n",
    "    compteur = 0\n",
    "    compte_publisUps = 0\n",
    "    partenaire = 0\n",
    "    dataex = []\n",
    "    datafr = []\n",
    "    datapubli = []\n",
    "    unique_org_ex = {}\n",
    "    unique_org_fr = {}\n",
    "    # Exclure les DOM-TOM français\n",
    "    France_et_dom_tom_codes = ['FR','GP', 'RE', 'MQ', 'GF', 'YT', 'PM', 'WF', 'TF', 'NC', 'PF']\n",
    "\n",
    "\n",
    "    namespaces = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
    "\n",
    "\n",
    "    #######################################\n",
    "    # La requête est lancée en boucle et obtient un résultat (une notice) à chaque fois\n",
    "    # chaque résultat est traité dans la boucle \"while\"\n",
    "    ########################################\n",
    "    while cursor_mark != previous_cursor_mark:\n",
    "        # Mise à jour du cursorMark\n",
    "        params[\"cursorMark\"] = cursor_mark\n",
    "        #print(f\"CursorMark: {cursor_mark}\")\n",
    "        compteur += 1\n",
    "        if compteur % 5000 == 0 or compteur == 1:\n",
    "            print(compteur)\n",
    "            if compteur % 5000 == 0:\n",
    "                # Convertir les données collectées pour les organismes étrangers\n",
    "                dataex = list(unique_org_ex.values())\n",
    "                df_ex = pd.DataFrame(dataex)\n",
    "                # Convertir les données collectées pour les organismes français\n",
    "                datafr = list(unique_org_fr.values())\n",
    "                df_fr = pd.DataFrame(datafr)\n",
    "\n",
    "                # Liste des publications/logiciels de HAL\n",
    "                df_publis = pd.DataFrame(datapubli)\n",
    "                # df_ex.to_excel(f\"df_ex_{compteur}.xlsx\", index=False)\n",
    "                # df_fr.to_excel(f\"df_fr_{compteur}.xlsx\", index=False)\n",
    "                # df_publis.to_excel(f\"df_publis_{compteur}.xlsx\", index=False)\n",
    "            time.sleep(2)  # Pause de 2 secondes entre les requêtes\n",
    "    \n",
    "        # Limite pour tests\n",
    "        # if compteur > 15:\n",
    "        #     break\n",
    "\n",
    "        response = fetch_with_retry(base_url, params)\n",
    "        if response:\n",
    "            try:\n",
    "                tree = etree.fromstring(response.content)\n",
    "            except etree.XMLSyntaxError:\n",
    "                print(\"Erreur de syntaxe XML. Réponse non analysée.\")\n",
    "                continue\n",
    "\n",
    "            # Récupération de la valeur de next dans l'attribut de la première balise TEI\n",
    "            next_cursor_mark = tree.attrib.get(\"next\")\n",
    "            # print(next_cursor_mark)\n",
    "            \n",
    "            # indication du nombre de notices répondant à la requête\n",
    "            quantity_value = tree.find('.//tei:measure', namespaces=namespaces).attrib.get('quantity')\n",
    "            \n",
    "            if cursor_mark == \"*\":\n",
    "                # seulement lors de la première boucle, on indique le nombre total de notices répondant à la requête\n",
    "                print(f\"nbre résultats : {quantity_value}. Durée estimée pour 4000 notices : 20 mn\")\n",
    "\n",
    "            # identification de la notice dans le xml-tei pour trouver les métadonnées\n",
    "            biblfull_elements = tree.findall('.//tei:biblFull', namespaces=namespaces)\n",
    "\n",
    "            if biblfull_elements:\n",
    "                biblfull = biblfull_elements[0]  # Get the first matching element\n",
    "            else:\n",
    "                biblfull = None  # Handle the absence of the element\n",
    "                if cursor_mark == next_cursor_mark:\n",
    "                    print(f\"ancien curseur : {cursor_mark} - nouveau curseur : {next_cursor_mark}\")\n",
    "                    print(\"pas de biblFull - Terminé\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Il y a eu un problème dans la réponse de HAL, veuillez relancer le script\")\n",
    "                    break\n",
    "\n",
    "            # Récupération des metadonnées\n",
    "            if biblfull is not None:\n",
    "                #identifiant de la publication dans HAL\n",
    "                halID = biblfull.xpath('.//tei:publicationStmt/tei:idno[@type=\"halId\"]/text()', namespaces=namespaces) or [\"pas de hal_ID\"]\n",
    "                halID_value = halID[0] if halID else \"no HalID\"\n",
    "                # print(halID_value)\n",
    "                \n",
    "                # Domaines = biblfull.xpath('.//tei:profileDesc/tei:textClass/tei:classCode[@scheme=\"halDomain\"]/text()', namespaces=namespaces)\n",
    "                # Domaines_value = \";\".join(domaine.strip() for domaine in Domaines if domaine)\n",
    "\n",
    "\n",
    "                # TRAITEMENT DES AFFILIATIONS contenues dans la notice\n",
    "                orgs = tree.findall('.//tei:listOrg[@type=\"structures\"]/tei:org', namespaces=namespaces)\n",
    "\n",
    "                for org in orgs:\n",
    "                    xml_id = org.xpath('@xml:id', namespaces=namespaces) # code de la structure\n",
    "                    lenom = org.xpath('.//tei:orgName/text()', namespaces=namespaces)\n",
    "                    lacronyme = org.xpath('.//tei:orgName[@type=\"acronym\"]/text()', namespaces=namespaces)\n",
    "                    lepays = org.xpath('.//tei:country/@key', namespaces=namespaces)\n",
    "                    ladresse = [addr.text for addr in org.xpath('.//tei:addrLine', namespaces=namespaces) if addr.text]\n",
    "                    ladresse_value = \" \".join(ladresse)\n",
    "                    lesrelations = org.xpath('.//tei:listRelation/tei:relation/@active', namespaces=namespaces) # codes des structures parentes\n",
    "\n",
    "                    # Supprimer '#struct-' de chaque élément de la liste\n",
    "                    lesrelations_cleaned = [relation.replace('#struct-', '') for relation in lesrelations]\n",
    "                    xml_id_cleaned = xml_id[0].lstrip('struct-') \n",
    "                    \n",
    "                    # si l'identifiant structure fait partie des identifiants à exclure on passe au suivant sans traiter.\n",
    "                    # if xml_id_cleaned in equipes_a_exclure:\n",
    "                    #     continue\n",
    "\n",
    "                    # Organismes copubliants non français\n",
    "                    if lepays and lepays[0] not in France_et_dom_tom_codes:\n",
    "\n",
    "                        # print (f\"{xml_id_cleaned} trouvé\")\n",
    "                        partenaire = 1 # à noter qu'il peut s'agir d'une structure mère étrangère qui ne va pas entrer en compte au final\n",
    "                        unique_org_ex[xml_id[0]] = {\n",
    "                            \"Pays_ex\": lepays,  # Le pays (on filtrera ensuite)\n",
    "                            \"OrganismeEx\": lenom[0],  # Les noms des institutions\n",
    "                            \"ID_aurehal\": xml_id_cleaned,  # L'attribut xml:id\n",
    "                            \"adresse\": ladresse_value,\n",
    "                            \"parents\": lesrelations_cleaned\n",
    "\n",
    "                        }\n",
    "                    # Organisme FR\n",
    "                    elif lepays and lepays[0] in France_et_dom_tom_codes:\n",
    "\n",
    "                        #print(lepays)\n",
    "                        unique_org_fr[xml_id[0]] = {\n",
    "                            \"Pays_fr\": lepays,  # Le pays\n",
    "                            \"Organisme_fr\": lenom[0],  # Les noms des institutions\n",
    "                            \"Acronyme_fr\": lacronyme[0] if lacronyme else 'na',\n",
    "                            \"ID_aurehal\": xml_id_cleaned,  # L'attribut xml:id\n",
    "                            \"adresse\": ladresse_value,\n",
    "                            \"parents\": lesrelations_cleaned\n",
    "\n",
    "                        }\n",
    "                \n",
    "                # Si on veut limiter les résultats aux publications avec des copubliants internationaux alors il faut décommenter la ligne suivante      \n",
    "                if partenaire == 1: # si on a trouvé un pays hors FR\n",
    "            \n",
    "                # et il faut décaler les lignes suivantes aussi\n",
    "                # Sinon, on prend toutes les publications (par ex, si on veut calculer la proportion de copublications avec l'étranger par rapport au total)\n",
    "\n",
    "                    # Récupération de l'année de publication   \n",
    "                    \n",
    "                    date_value = biblfull.xpath('.//tei:sourceDesc/tei:biblStruct//tei:monogr/tei:imprint/tei:date[@type=\"datePub\"]/text()', namespaces=namespaces)\n",
    "                    date_produced = biblfull.xpath('.//tei:editionStmt/tei:edition/tei:date[@type=\"whenProduced\"]/text()', namespaces=namespaces)\n",
    "                    if date_value and date_value is not None:\n",
    "                        date_text = date_value[0]  # Récupérer la chaîne de date\n",
    "                        year_value = date_text[:4]  # Les 4 premiers caractères pour l'année\n",
    "                    else:\n",
    "                        year_value = date_produced[0][:4]\n",
    "\n",
    "                    keywords = biblfull.xpath('.//tei:profileDesc/tei:textClass/tei:keywords/tei:term', namespaces=namespaces)\n",
    "                    # Extraire les mots-clés et joindre avec \";\"\n",
    "                    keywords_str = \";\".join(\n",
    "                        \" \".join(term.text.split())  # supprime espaces multiples et trims\n",
    "                        for term in keywords\n",
    "                        if term.text\n",
    "                    )\n",
    "                    # Récupérer tous les <classCode> avec scheme=\"halDomain\"\n",
    "                    hal_domain_elems = biblfull.xpath(\n",
    "                        './/tei:profileDesc/tei:textClass/tei:classCode[@scheme=\"halDomain\"]',\n",
    "                        namespaces=namespaces\n",
    "                    )\n",
    "\n",
    "                    # Nettoyer et joindre avec \";\"\n",
    "                    if hal_domain_elems:\n",
    "                        hal_domain_str = \";\".join(\n",
    "                            elem.text.strip() for elem in hal_domain_elems if elem.text\n",
    "                        )\n",
    "                    else:\n",
    "                        hal_domain_str = \"\"\n",
    "                        \n",
    "                    # Récupérer <abstract> en priorité \"en\", sinon \"fr\", sinon chaîne vide\n",
    "                    def get_full_text(elem):\n",
    "                        return \"\".join(elem.itertext()).strip() if elem is not None else \"\"\n",
    "                    abstract_elem = biblfull.xpath(\n",
    "                        './/tei:profileDesc/tei:abstract[@xml:lang=\"en\"]',\n",
    "                        namespaces=namespaces\n",
    "                    )\n",
    "                    if not abstract_elem:  # fallback en \"fr\"\n",
    "                        abstract_elem = biblfull.xpath(\n",
    "                            './/tei:profileDesc/tei:abstract[@xml:lang=\"fr\"]',\n",
    "                            namespaces=namespaces\n",
    "                        )\n",
    "                    abstract_str = get_full_text(abstract_elem[0]) if abstract_elem else \"\"\n",
    "                    \n",
    "                    # titre_revue = titre_journal[0].text if titre_journal  else \"\"\n",
    "                    # # print(titre_revue)\n",
    "                    # conference_titles = biblfull.xpath('.//tei:sourceDesc/tei:biblStruct/tei:monogr/tei:meeting/tei:title', namespaces=namespaces)\n",
    "                    # titre_conf = conference_titles[0].text if conference_titles else \"\"\n",
    "                    # print(titre_conf)\n",
    "            \n",
    "                \n",
    "                # Identification des affiliations associées à chaque auteur\n",
    "                    for author in biblfull.xpath('.//tei:titleStmt/tei:author', namespaces=namespaces):\n",
    "                        forename = author.xpath('.//tei:persName/tei:forename/text()', namespaces=namespaces)  or [\"Unknown\"]\n",
    "                        surname = author.xpath('.//tei:persName/tei:surname/text()', namespaces=namespaces)  or [\"Unknown\"]\n",
    "                                        \n",
    "                        authorLastFirstnames = (f\"{surname[0]}, {forename[0]}\")\n",
    "\n",
    "                        affiliations = author.xpath('.//tei:affiliation/@ref', namespaces=namespaces)\n",
    "\n",
    "                        for affiliation in affiliations:\n",
    "                            affiliation = affiliation.lstrip('#struct-')\n",
    "\n",
    "                            # si l'identifiant structure fait partie des identifiants à exclure on passe au suivant sans traiter.\n",
    "                            # if affiliation in equipes_a_exclure:\n",
    "                            #     continue\n",
    "\n",
    "                            datapubli.append ({\n",
    "                                \"halID\" : halID_value,\n",
    "                                \"Auteur\" : authorLastFirstnames,\n",
    "                                \"affiliation\" : affiliation,\n",
    "                                \"Annee\" : year_value,\n",
    "                                \"MotsCles\": keywords_str,\n",
    "                                \"Domaine(s)\":hal_domain_str,\n",
    "                                \"Resume\":abstract_str,\n",
    "                            })\n",
    "                    partenaire = 0\n",
    "\n",
    "                #print(f\"{halID_value} - {authorLastFirstnames} - {affiliation}\")\n",
    "                    \n",
    "                # En cas de récupération intensive de données, forcer la libération de la mémoire\n",
    "                # gc.collect()\n",
    "\n",
    "                # Mise à jour du cursorMark pour la prochaine itération\n",
    "                previous_cursor_mark = cursor_mark\n",
    "                cursor_mark = next_cursor_mark\n",
    "                # Pause pour éviter de surcharger l'API\n",
    "                # time.sleep(0.1)\n",
    "\n",
    "                if not next_cursor_mark:\n",
    "                    print(compteur)\n",
    "                    break\n",
    "\n",
    "    # Ajoute les résultats cumulés\n",
    "    all_dataex.update(unique_org_ex)\n",
    "    all_datafr.update(unique_org_fr)\n",
    "    all_datapubli.extend(datapubli)\n",
    "\n",
    "print(\"Terminé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64bfad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Conversion en \"dataframes\" pour traitement des données et comptage\n",
    "######################################################################\n",
    "\n",
    "df_ex = pd.DataFrame(list(all_dataex.values()))\n",
    "df_fr = pd.DataFrame(list(all_datafr.values()))\n",
    "df_publis = pd.DataFrame(all_datapubli)\n",
    "\n",
    "# Sauvegarde pour contrôle et tests\n",
    "# df_ex.to_excel(\"df_ex_total.xlsx\", index=False)\n",
    "# df_fr.to_excel(\"df_fr_total.xlsx\", index=False)\n",
    "# df_publis.to_excel(\"df_publis_total.xlsx\", index=False)\n",
    "print(\"dataframes créés\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pour tests : df_publis=pd.read_excel(\"df_publis_total.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67dd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_publis.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################\n",
    "# FILTRE UE / hors UE pour df_ex\n",
    "####################################\n",
    "df_nonUE = \"\"\n",
    "df_UE = \"\"\n",
    "pays_UE = [\n",
    "    \"AT\", \"BE\", \"BG\", \"HR\", \"CY\", \"CZ\", \"DK\", \"EE\", \"FI\", \"FR\",\n",
    "    \"DE\", \"GR\", \"HU\", \"IE\", \"IT\", \"LV\", \"LT\", \"LU\", \"MT\", \"NL\",\n",
    "    \"PL\", \"PT\", \"RO\", \"SK\", \"SI\", \"ES\", \"SE\"\n",
    "]\n",
    "\n",
    "# On va créer 2 dataframes en fonction du pays de la structure (UE ou hors UE)\n",
    "#\n",
    "\n",
    "# Fonction pour vérifier si une valeur de la liste est dans la colonne `identifiant` ou `parents`\n",
    "def filter_rows_EU(row):\n",
    "    # Vérifie si une des valeurs de la liste se trouve dans `country` ou dans les valeurs de `parents`\n",
    "    return any(country in pays_UE for country in row[\"Pays_ex\"])\n",
    "\n",
    "def filter_rows_ex(row):\n",
    "    # Vérifie si une des valeurs de la liste ne se trouve pas dans `country` ou dans les valeurs de `parents`\n",
    "    return any(country not in pays_UE for country in row[\"Pays_ex\"])\n",
    "\n",
    "\n",
    "# Filtrer les lignes du DataFrame pour garder celles des structures qui nous intéressent\n",
    "df_UE = df_ex[df_ex.apply(filter_rows_EU, axis=1)]\n",
    "df_UE.rename(columns={\"OrganismeEx\" : \"Organisme_UE\"}, inplace=True)\n",
    "\n",
    "\n",
    "df_nonUE = df_ex[df_ex.apply(filter_rows_ex, axis=1)]\n",
    "df_nonUE.rename(columns={\"OrganismeEx\" : \"Organisme_Hors_UE\"}, inplace=True)\n",
    "\n",
    "\n",
    "print(\"Dataframes UE et non UE créés\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a09ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Interprétation des codes Pays en noms en toutes lettres \n",
    "###########################################################\n",
    "\n",
    "# Récupérer les données de l'API\n",
    "# url = \"https://restcountries.com/v3.1/all\"\n",
    "# response = requests.get(url)\n",
    "# countries_data = response.json()\n",
    "import requests\n",
    "\n",
    "def get_country_mapping():\n",
    "    url = \"https://restcountries.com/v3.1/all\"\n",
    "    params = {\"fields\": \"cca2,name\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            countries_data = response.json()\n",
    "            if isinstance(countries_data, list):\n",
    "                return {\n",
    "                    country.get(\"cca2\"): country.get(\"name\", {}).get(\"common\")\n",
    "                    for country in countries_data\n",
    "                    if country.get(\"cca2\") and \"name\" in country and \"common\" in country[\"name\"]\n",
    "                }\n",
    "            else:\n",
    "                print(\"⚠️ Format inattendu :\", type(countries_data))\n",
    "                return {}\n",
    "        else:\n",
    "            print(f\"⚠️ Erreur API ({response.status_code}): {response.json().get('message')}\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(\"❌ Erreur lors de la récupération des données pays :\", e)\n",
    "        return {}\n",
    "\n",
    "# Utilisation\n",
    "country_mapping = get_country_mapping()\n",
    "print(\"✅ Exemple : FR →\", country_mapping.get(\"FR\"))  # Affiche 'France'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_UE[\"Pays_ex\"] = df_UE[\"Pays_ex\"].apply(lambda x: country_mapping.get(x[0]) if isinstance(x, list) and x else x)\n",
    "df_UE['TypePays'] = \"EU\"\n",
    "\n",
    "df_nonUE[\"Pays_ex\"] = df_nonUE[\"Pays_ex\"].apply(lambda x: country_mapping.get(x[0]) if isinstance(x, list) and x else x)\n",
    "df_nonUE['TypePays'] = \"EX\"\n",
    "\n",
    "df_fr[\"Pays_fr\"] = df_fr[\"Pays_fr\"].apply(lambda x: country_mapping.get(x[0]) if isinstance(x, list) and x else x)\n",
    "df_fr['TypePays'] = \"FR\"\n",
    "\n",
    "\n",
    "# Afficher un aperçu du DataFrame modifié\n",
    "print(df_UE.head(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## \n",
    "# Ajouter l'acronyme des auteurs Inria dans la liste des publications\n",
    "###########\n",
    "\n",
    "# Charger ton fichier d'équipes Inria\n",
    "df_equipes = pd.read_excel(\"equipesInriadeAurehal.xlsx\")\n",
    "\n",
    "# Vérifie que les colonnes utiles existent\n",
    "assert \"docid\" in df_equipes.columns, \"Colonne 'docid' manquante dans le fichier Excel\"\n",
    "assert \"acronyme\" in df_equipes.columns, \"Colonne 'acronyme' manquante dans le fichier Excel\"\n",
    "\n",
    "# S'assurer que 'docid' et 'affiliation' sont bien de type str\n",
    "df_equipes[\"docid\"] = df_equipes[\"docid\"].astype(str)\n",
    "df_publis[\"affiliation\"] = df_publis[\"affiliation\"].astype(str)\n",
    "\n",
    "# Fusion des deux DataFrames : on ajoute l'acronyme en fonction de l'affiliation\n",
    "df_publis = df_publis.merge(df_equipes[[\"docid\", \"acronyme\"]], how=\"left\", left_on=\"affiliation\", right_on=\"docid\")\n",
    "\n",
    "# Optionnel : supprimer la colonne docid (redondante après le merge)\n",
    "df_publis.drop(columns=[\"docid\"], inplace=True)\n",
    "\n",
    "# Exemple d'affichage\n",
    "print(df_publis[[\"halID\", \"Auteur\", \"affiliation\", \"acronyme\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pour tests df_publis[df_publis[\"halID\"] == \"hal-00799242\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage : Pour les auteurs Inria (auteurs ayant un acronyme), supprimer les autres affiliations\n",
    "# Marquer les cas où pour chaque (halID, Auteur), au moins un acronyme est non-null\n",
    "df=df_publis\n",
    "\n",
    "df[\"has_acronyme\"] = df.groupby([\"halID\", \"Auteur\"])[\"acronyme\"].transform(lambda x: x.notna().any())\n",
    "\n",
    "# Ne garder que :\n",
    "# - les lignes où acronyme est non-null\n",
    "# - ou les cas où aucune ligne pour ce (halID, Auteur) n'a d'acronyme\n",
    "df = df[(df[\"acronyme\"].notna()) | (~df[\"has_acronyme\"])]\n",
    "\n",
    "# Supprimer la colonne temporaire\n",
    "df = df.drop(columns=[\"has_acronyme\"])\n",
    "\n",
    "df_publis = df\n",
    "df_publis.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab58e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter l'organisme et le pays UE des auteurs à la liste générale des publications\n",
    "\n",
    "# Vérifier que les colonnes existent\n",
    "assert \"ID_aurehal\" in df_UE.columns, \"Colonne 'ID_aurehal' manquante dans df_UE\"\n",
    "assert \"Organisme_UE\" in df_UE.columns, \"Colonne 'Organisme_UE' manquante dans df_UE\"\n",
    "assert \"Pays_ex\" in df_UE.columns, \"Colonne 'Pays_ex' manquante dans df_UE\"\n",
    "assert \"adresse\" in df_UE.columns, \"Colonne 'adresse' manquante dans df_UE\"\n",
    "\n",
    "# Harmoniser les types de colonnes pour le merge\n",
    "df_UE[\"ID_aurehal\"] = df_UE[\"ID_aurehal\"].astype(str)\n",
    "df_publis[\"affiliation\"] = df_publis[\"affiliation\"].astype(str)\n",
    "\n",
    "# Renommer les colonnes de df_nonUE pour éviter les collisions\n",
    "df_UE_renamed = df_UE.rename(columns={\n",
    "    \"adresse\": \"adresse_UE\",  # Pour éviter d’écraser la précédente\n",
    "})\n",
    "\n",
    "# Faire la jointure\n",
    "df_publis = df_publis.merge(\n",
    "    df_UE_renamed[[\"ID_aurehal\", \"Organisme_UE\", \"Pays_ex\",\"adresse_UE\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"affiliation\",\n",
    "    right_on=\"ID_aurehal\"\n",
    ")\n",
    "\n",
    "# Supprimer la colonne intermédiaire redondante\n",
    "df_publis.drop(columns=[\"ID_aurehal\"], inplace=True)\n",
    "\n",
    "# Vérification du résultat\n",
    "print(df_publis[[\"affiliation\", \"Organisme_UE\", \"Pays_ex\",\"adresse_UE\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962cb9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonUE.head(1) #pour connaître le nom des colonnes afin de faire le traitement suivant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7fc718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter l'organisme et le pays Hors UE des auteurs\n",
    "\n",
    "# Vérifier que les colonnes existent\n",
    "assert \"ID_aurehal\" in df_nonUE.columns, \"Colonne 'ID_aurehal' manquante dans df_nonUE\"\n",
    "assert \"Organisme_Hors_UE\" in df_nonUE.columns, \"Colonne 'Organisme_Hors_UE' manquante dans df_nonUE\"\n",
    "assert \"Pays_ex\" in df_nonUE.columns, \"Colonne 'Pays_ex' manquante dans df_nonUE\"\n",
    "assert \"adresse\" in df_nonUE.columns, \"Colonne 'adresse' manquante dans df_nonUE\"\n",
    "\n",
    "# Harmoniser les types de colonnes pour le merge\n",
    "df_nonUE[\"ID_aurehal\"] = df_nonUE[\"ID_aurehal\"].astype(str)\n",
    "df_publis[\"affiliation\"] = df_publis[\"affiliation\"].astype(str)\n",
    "\n",
    "# Renommer les colonnes de df_nonUE pour éviter les collisions\n",
    "df_nonUE_renamed = df_nonUE.rename(columns={\n",
    "    \"Pays_ex\": \"Pays_ex_horsUE\",  # Pour éviter d’écraser la précédente\n",
    "    \"adresse\": \"adresse_hors_UE\", \n",
    "})\n",
    "\n",
    "# Faire la jointure\n",
    "df_publis = df_publis.merge(\n",
    "    df_nonUE_renamed[[\"ID_aurehal\", \"Organisme_Hors_UE\", \"Pays_ex_horsUE\",\"adresse_hors_UE\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"affiliation\",\n",
    "    right_on=\"ID_aurehal\"\n",
    ")\n",
    "\n",
    "# Supprimer la colonne intermédiaire redondante\n",
    "df_publis.drop(columns=[\"ID_aurehal\"], inplace=True)\n",
    "\n",
    "# Vérification du résultat\n",
    "print(df_publis[[\"affiliation\", \"Organisme_Hors_UE\", \"Pays_ex_horsUE\",\"adresse_hors_UE\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  contrôle pour tests df_publis[df_publis[\"halID\"] == \"hal-00799242\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0381c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Supprimer les auteurs qui ont à la fois une affiliation française et une affiliation étrangère)\n",
    "#########################################\n",
    "# Indicateurs\n",
    "df_publis[\"has_org\"] = df_publis[\"Organisme_UE\"].notna() | df_publis[\"Organisme_Hors_UE\"].notna()\n",
    "df_publis[\"has_no_acronyme\"] = df_publis[\"acronyme\"].isna() | (df_publis[\"acronyme\"].str.strip() == \"\")\n",
    "\n",
    "# Grouper par halID + Auteur\n",
    "grouped = df_publis.groupby([\"halID\", \"Auteur\"]).agg(\n",
    "    n_lignes=(\"halID\", \"count\"),          # nombre de lignes pour ce couple\n",
    "    has_no_acronyme=(\"has_no_acronyme\", \"any\"),\n",
    "    has_org=(\"has_org\", \"any\")\n",
    ").reset_index()\n",
    "\n",
    "# On garde uniquement ceux qui ont au moins 2 lignes et les deux cas\n",
    "auteurs_mixtes = grouped[\n",
    "    (grouped[\"n_lignes\"] >= 2) &\n",
    "    (grouped[\"has_no_acronyme\"]) &\n",
    "    (grouped[\"has_org\"])\n",
    "]\n",
    "\n",
    "# Supprimer ces auteurs de df_publis\n",
    "df_publis_clean = df_publis.merge(\n",
    "    auteurs_mixtes[[\"halID\", \"Auteur\"]],\n",
    "    on=[\"halID\", \"Auteur\"],\n",
    "    how=\"left\",\n",
    "    indicator=True\n",
    ")\n",
    "df_publis_clean = df_publis_clean[df_publis_clean[\"_merge\"] == \"left_only\"].drop(columns=\"_merge\")\n",
    "\n",
    "print(f\"✅ {len(df_publis) - len(df_publis_clean)} lignes supprimées\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test df_publis_clean[df_publis_clean[\"halID\"] == \"hal-00799242\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test df_publis_clean[df_publis_clean[\"halID\"] == \"hal-01895279\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefbfa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_publis = df_publis_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd359ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage : On supprime tous les co-auteurs français = ne sont pas Inria (pas d'acronyme) et n'ont pas d'affiliations étrangères\n",
    "\n",
    "# Colonnes à tester pour le vide\n",
    "cols_to_check = [\"Organisme_UE\", \"Organisme_Hors_UE\", \"acronyme\"]\n",
    "\n",
    "# Masque des lignes vides\n",
    "mask_empty = df_publis[cols_to_check].apply(\n",
    "    lambda row: all(pd.isna(v) or str(v).strip() == \"\" for v in row),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# On garde seulement les lignes qui ne sont pas \"vides\"\n",
    "df_publis_clean = df_publis[~mask_empty].copy()\n",
    "\n",
    "print(f\"✅ {mask_empty.sum()} lignes supprimées\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0baa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test df_publis_clean[df_publis_clean[\"halID\"] == \"hal-00799242\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4909a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test df_publis_clean[df_publis_clean[\"halID\"] == \"cea-04228169\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deedb92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_publis = df_publis_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_publis.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c48a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ne garde pas les publications avec juste un seul auteur\n",
    "df_publis_clean = df_publis[df_publis.groupby(\"halID\")[\"halID\"].transform(\"count\") > 1].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4fbaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test df_publis_clean[df_publis_clean[\"halID\"] == \"hal-00799242\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test df_publis_clean[df_publis_clean[\"halID\"] == \"cea-04228169\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afecece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_publis = df_publis_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be216cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fichier final (1e partie) avec , pour chaque auteur Inria, les copubliants étrangers\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. On part du df_publis et on isole les auteurs FR et étrangers\n",
    "auteurs_fr = df_publis[pd.notna(df_publis['acronyme']) & (df_publis['acronyme'].str.strip() != '')]\n",
    "auteurs_etr = df_publis[pd.isna(df_publis['acronyme']) | (df_publis['acronyme'].str.strip() == '')]\n",
    "\n",
    "# 2. Pour chaque auteur étranger, on veut rattacher les auteurs FR du même halID\n",
    "rows = []\n",
    "for _, row_etr in auteurs_etr.iterrows():\n",
    "    hal_id = row_etr['halID']\n",
    "    # Trouver les auteurs FR liés à ce halID\n",
    "    fr_list = auteurs_fr[auteurs_fr['halID'] == hal_id]\n",
    "    for _, row_fr in fr_list.iterrows():\n",
    "        rows.append({\n",
    "            'Equipe': row_fr['acronyme'],\n",
    "            'Auteurs FR': row_fr['Auteur'],\n",
    "            'Auteurs copubliants': row_etr['Auteur'],\n",
    "            'Organisme copubliant': row_etr['Organisme_Hors_UE'] if pd.notna(row_etr['Organisme_Hors_UE']) else row_etr['Organisme_UE'],\n",
    "            'Adresse': row_etr['adresse_hors_UE'] if pd.notna(row_etr['adresse_hors_UE']) else row_etr['adresse_UE'],\n",
    "            'Pays': row_etr['Pays_ex_horsUE'] if pd.notna(row_etr['Pays_ex_horsUE']) else row_etr['Pays_ex'],\n",
    "            'ID Aurehal': row_etr['affiliation'],\n",
    "            'Année': row_etr['Annee'],\n",
    "            'UE/Non UE': 'UE' if pd.notna(row_etr['Organisme_UE']) else 'Non UE',\n",
    "            'HalID': hal_id,\n",
    "            'Domaine(s)': row_etr['Domaine(s)'],\n",
    "            'Mots-cles' : row_etr['MotsCles'],\n",
    "            'Resume':row_etr['Resume'],\n",
    "        })\n",
    "\n",
    "# 3. Construire le DataFrame final\n",
    "df_final = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# 4. Trier par Equipe, Auteurs FR, Auteurs copubliants\n",
    "df_final = df_final.sort_values(by=['Equipe', 'Auteurs FR', 'Auteurs copubliants']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 5. Exporter vers Excel\n",
    "nom_du_fichier = f\"Copubliants_par_auteur_Inria_{nom_de_la_structure}.xlsx\"\n",
    "df_final.to_excel(nom_du_fichier, index=False)\n",
    "\n",
    "print(f\"✅ Fichier Excel créé : {nom_du_fichier}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb1a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test df_final[df_final[\"HalID\"] == \"hal-00799242\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Filtre sur le centre (pour supprimer lignes d'équipes d'autres centres copubliantes)\n",
    "####\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"je sélectionne les équipes de {nom_de_la_structure}\")\n",
    "\n",
    "# Charger ton mapping\n",
    "df_equipes = pd.read_excel(\"EquipesInriadeAurehal.xlsx\")\n",
    "\n",
    "# On garde seulement les équipes qui ont le nom de la structure dans parent_name\n",
    "equipes_centre = df_equipes[df_equipes[\"parent_name\"].str.contains(nom_de_la_structure, case=False, na=False)]\n",
    "\n",
    "# On récupère la liste des acronymes valides\n",
    "liste_acronymes = equipes_centre[\"acronyme\"].unique()\n",
    "\n",
    "# print(liste_acronymes)\n",
    "\n",
    "# Filtrer df_final pour ne garder que les Equipes présentes dans cette liste\n",
    "df_final_centre = df_final[df_final[\"Equipe\"].isin(liste_acronymes)]\n",
    "\n",
    "# Résultat\n",
    "print(df_final_centre.shape)\n",
    "df_final_centre.head()\n",
    "\n",
    "# Ajouter une colonne Centre\n",
    "df_final_centre[\"Centre\"] = nom_de_la_structure\n",
    "\n",
    "\n",
    "# 5. Exporter vers Excel\n",
    "\n",
    "df_final_centre.to_excel(f\"{nom_du_fichier}\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487de3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_final_centre.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00011110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrôle pour test print(nom_du_fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install geotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc502f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nom_du_fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea289437",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# fichier final avec ville :\n",
    "# repérage de la ville selon 3 méthodes successives:\n",
    "#1. le mot entre crochets du titre de la structure (en excluant les pays d'après une liste de pays courants)\n",
    "#2. Regex d'après le repérage d'un code postal de l'adresse \n",
    "#3. En faisant appel à l'analyse de l'adresse par le service Geotext\n",
    "#####################\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from geotext import GeoText\n",
    "import re\n",
    "\n",
    "# Charger ton fichier Excel initial\n",
    "df = pd.read_excel(nom_du_fichier)\n",
    "\n",
    "# Liste des pays à ignorer\n",
    "pays_a_ignorer = {\n",
    "    \"Algeria\", \"Argentina\", \"Australia\", \"Austria\", \"Belgium\", \"Bolivia\", \n",
    "    \"Bosnia and Herzegovina\", \"Brazil\", \"Brunei\", \"Bulgaria\", \"Burkina Faso\", \n",
    "    \"Cameroon\", \"Canada\", \"Chile\", \"China\", \"Colombia\", \"Costa Rica\", \"Croatia\", \n",
    "    \"Cyprus\", \"Czechia\", \"Denmark\", \"Ecuador\", \"Estonia\", \"Finland\", \"Georgia\", \n",
    "    \"Germany\", \"Greece\", \"Hong Kong\", \"Hungary\", \"Iceland\", \"India\", \"Indonesia\", \n",
    "    \"Iran\", \"Ireland\", \"Israel\", \"Italy\", \"Japan\", \"Jordan\", \"Kenya\", \"Latvia\", \n",
    "    \"Lebanon\", \"Lithuania\", \"Luxembourg\", \"Madagascar\", \"Malaysia\", \"Malta\", \n",
    "    \"Mexico\", \"Morocco\", \"Netherlands\", \"New Zealand\", \"Niger\", \"Nigeria\", \n",
    "    \"North Macedonia\", \"Norway\", \"Oman\", \"Pakistan\", \"Peru\", \"Poland\", \"Portugal\", \n",
    "    \"Romania\", \"Russia\", \"Saudi Arabia\", \"Senegal\", \"Serbia\", \"Singapore\", \n",
    "    \"Slovakia\", \"Slovenia\", \"South Africa\", \"South Korea\", \"Spain\", \"Sweden\", \n",
    "    \"Switzerland\", \"Taiwan\", \"Thailand\", \"Tunisia\", \"Turkey\", \"Uganda\", \"Ukraine\", \n",
    "    \"United Arab Emirates\", \"United Kingdom\", \"United States\", \"Uruguay\", \n",
    "    \"Venezuela\", \"Vietnam\"\n",
    "}\n",
    "\n",
    "# Fonction fusionnée\n",
    "def get_ville(organisme, adresse):\n",
    "    # 1. Vérifier crochets dans \"Organisme copubliant\"\n",
    "    if isinstance(organisme, str):\n",
    "        match = re.search(r\"\\[(.*?)\\]\", organisme)\n",
    "        if match:\n",
    "            contenu = match.group(1).strip()\n",
    "            if contenu not in pays_a_ignorer:\n",
    "                return contenu\n",
    "    \n",
    "    # 2. Sinon regex sur \"Adresse\"\n",
    "    if isinstance(adresse, str):\n",
    "        match = re.search(r\"\\b(\\d{4,5})\\s+([A-Za-zÀ-ÖØ-öø-ÿ\\- ]+)\", adresse)\n",
    "        if match:\n",
    "            city = match.group(2).strip()\n",
    "            city = re.sub(r\"[^A-Za-zÀ-ÖØ-öø-ÿ\\- ]\", \"\", city)\n",
    "            if city:\n",
    "                return city\n",
    "\n",
    "        # 3. Sinon GeoText\n",
    "        places = GeoText(adresse)\n",
    "        if places.cities:\n",
    "            return places.cities[0]\n",
    "\n",
    "    return None\n",
    "\n",
    "# Appliquer la fonction\n",
    "df[\"Ville\"] = df.apply(lambda row: get_ville(row[\"Organisme copubliant\"], row[\"Adresse\"]), axis=1)\n",
    "\n",
    "# Réordonner les colonnes\n",
    "cols_order = [\n",
    "    'Centre','Equipe','Auteurs FR', 'Auteurs copubliants', 'Organisme copubliant',\n",
    "    'Adresse', 'Ville', 'Pays', 'ID Aurehal', 'UE/Non UE', 'Année',\n",
    "    'HalID','Domaine(s)','Mots-cles','Resume'\n",
    "]\n",
    "df = df[cols_order]\n",
    "\n",
    "# Sauvegarder le fichier final\n",
    "df.to_excel(f\"Copubliants_par_auteur_Inria_{nom_de_la_structure}_ville_final.xlsx\", index=False)\n",
    "\n",
    "print(\"✅ Fichier créé avec détection Ville (crochets → regex → GeoText)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a930e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36923fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rendre les valeurs cliquables dans fichier excel\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "nomdufichier = f\"Copubliants_par_auteur_Inria_{nom_de_la_structure}_ville_final.xlsx\"\n",
    "\n",
    "\n",
    "# Charger le fichier Excel\n",
    "wb = load_workbook(nomdufichier)\n",
    "ws = wb.active\n",
    "\n",
    "# Définir une fonction pour créer les liens\n",
    "def create_link_hal(hal):\n",
    "    return f'https://inria.hal.science/{hal}'\n",
    "\n",
    "def create_link_aurehal(aurehal):\n",
    "    return f'https://aurehal.archives-ouvertes.fr/structure/read/id/{aurehal}' # lien vers la structure\n",
    "\n",
    "def create_link_revconf_conf(revconf):\n",
    "    return f'https://revconf.inria.fr/conference/view/{revconf}'\n",
    "\n",
    "def create_link_revconf_journal(revconf_journal):\n",
    "    return f'https://revconf.inria.fr/journal/view/{revconf_journal}'\n",
    "\n",
    "def create_link_revconf_editeur(revconf_editeur):\n",
    "    return f'https://revconf.inria.fr/publisher/view/{revconf_editeur}'\n",
    "\n",
    "def create_link_monitor(monitor):\n",
    "    return f'https://monitor.hal.science/?structure={monitor}&publicationDate=2020-2024'\n",
    "\n",
    "# Parcourir les cellules de la colonne 'n' (même valeur 'n' pour min_col et max_col qui est la colonne où se trouve l'identifiant ) et ajouter les liens hypertexte\n",
    "for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=12, max_col=12):\n",
    "    for cell in row:\n",
    "        hal = cell.value\n",
    "        if hal:\n",
    "            cell.hyperlink = create_link_hal(hal)\n",
    "print (\"liens hal terminé\")\n",
    "\n",
    "for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=9, max_col=9):\n",
    "    for cell in row:\n",
    "        aurehal = cell.value\n",
    "        if aurehal:\n",
    "            cell.hyperlink = create_link_aurehal(aurehal)\n",
    "print (\"liens aurehal terminé\")\n",
    "\n",
    "\n",
    "# Enregistrer les modifications dans le fichier Excel\n",
    "wb.save(nomdufichier)\n",
    "\n",
    "print(\"ouf, terminé 2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
